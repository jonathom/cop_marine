{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "\n",
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import ftplib\n",
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the merged and preprocessed data\n",
    "data = pd.read_csv('data/preprocessed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple model: linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target \n",
    "y = data.SOG_norm\n",
    "\n",
    "# predictors\n",
    "features = ['VHM0_norm','VMDR_norm','Temperature_norm','Salinity_norm'] \n",
    "X = data[features]\n",
    "\n",
    "# split data to train and validation\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(train_X, train_y)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "pred_y = regr.predict(val_X)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "# The mean squared error\n",
    "print('Mean squared error: %.2f'\n",
    "      % mean_squared_error(val_y, pred_y))\n",
    "# R2: 1 is perfect prediction\n",
    "print('R^2: %.2f'\n",
    "      % r2_score(val_y, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediate model: Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "from pathlib import Path\n",
    "\n",
    "my_file = Path(\"data/rf_model.joblib\")\n",
    "if my_file.is_file():\n",
    "    forest_model = load('data/rf_model.joblib') \n",
    "else:\n",
    "    forest_model = RandomForestRegressor(random_state=1)\n",
    "    forest_model.fit(train_X, train_y)\n",
    "    pred_y = forest_model.predict(val_X)\n",
    "    # The mean squared error\n",
    "    print('Mean squared error: %.2f'\n",
    "          % mean_squared_error(val_y, pred_y))\n",
    "    # R2: 1 is perfect prediction\n",
    "    print('R^2: %.2f'\n",
    "          % r2_score(val_y, pred_y))\n",
    "    dump(forest_model, 'data/rf_model.joblib') \n",
    "\n",
    "#Add variable importance\n",
    "feature_importance_values = forest_model.feature_importances_\n",
    "feature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(df):\n",
    "    \"\"\"Plot importances returned by a model. This can work with any measure of\n",
    "    feature importance provided that higher importance is better. \n",
    "    \n",
    "    Args:\n",
    "        df (dataframe): feature importances. Must have the features in a column\n",
    "        called `features` and the importances in a column called 'importance'\n",
    "    Returns:\n",
    "        shows a plot of the 15 most importance features\n",
    "        \n",
    "        df (dataframe): feature importances sorted by importance (highest to lowest) \n",
    "        with a column for normalized importance\n",
    "        \"\"\"\n",
    "    \n",
    "    # Sort features according to importance\n",
    "    df = df.sort_values('importance', ascending = False).reset_index()\n",
    "    \n",
    "    # Normalize the feature importances to add up to one\n",
    "    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n",
    "\n",
    "    # Make a horizontal bar chart of feature importances\n",
    "    plt.figure(figsize = (10, 6))\n",
    "    ax = plt.subplot()\n",
    "    \n",
    "    # Need to reverse the index to plot most important on top\n",
    "    ax.barh(list(reversed(list(df.index[:15]))), \n",
    "            df['importance_normalized'].head(15), \n",
    "            align = 'center', edgecolor = 'k')\n",
    "    \n",
    "    # Set the yticks and labels\n",
    "    ax.set_yticks(list(reversed(list(df.index[:15]))))\n",
    "    ax.set_yticklabels(df['feature'].head(15))\n",
    "    \n",
    "    # Plot labeling\n",
    "    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n",
    "    plt.show()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Show the feature importances for the default features\n",
    "feature_importances_sorted = plot_feature_importances(feature_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join all wave products by using open_mfdataset, chunking data in response to memory issues\n",
    "wav_all = xr.open_mfdataset('data/routing/mf*.nc')\n",
    "wav_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phy_all = xr.open_mfdataset('data/routing/me*.nc')\n",
    "phy_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the optimal shipping route between Lisbon and Rio de Janeiro avoiding high waves.\n",
    "\n",
    "Lisbon: 38.716666째 N 9.1667째 W\n",
    "\n",
    "Rio de Janeiro: 22.908333째 S 43.196389째 W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get array index to the value that is closest to a given value\n",
    "def get_closest(array, value):\n",
    "    return np.abs(array - value).argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set bounding box for the allowed routing corridor\n",
    "bbox = ((-45, -25),(-7, 41))\n",
    "# Select time\n",
    "time_slice_wav = 3\n",
    "time_slice_phy = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices of the bbox\n",
    "lon_min = get_closest(wav_all.longitude.data, bbox[0][0])\n",
    "lat_min = get_closest(wav_all.latitude.data, bbox[0][1])\n",
    "lon_max = get_closest(wav_all.longitude.data, bbox[1][0])\n",
    "lat_max = get_closest(wav_all.latitude.data, bbox[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Very simple solution: Calculate optimal route (minimum cost path) based on one variable _wave height_ for _one_ day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract array from dataset to define the cost in the routing algorithm \n",
    "# Wave height\n",
    "wave_height = wav_all.VHM0.isel(time=time_slice_wav, longitude=slice(lon_min, lon_max), latitude=slice(lat_min, lat_max))\n",
    "\n",
    "# Wave direction\n",
    "wave_dir = wav_all.VMDR.isel(time=time_slice_wav, longitude=slice(lon_min, lon_max), latitude=slice(lat_min, lat_max))\n",
    "\n",
    "# Temperature\n",
    "temp = phy_all.thetao.isel(time=time_slice_phy, longitude=slice(lon_min, lon_max), latitude=slice(lat_min, lat_max), depth = 0)\n",
    "\n",
    "# Salinity\n",
    "sal = phy_all.so.isel(time=time_slice_phy, longitude=slice(lon_min, lon_max), latitude=slice(lat_min, lat_max), depth = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start and end point of the route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_Lis = 38.716666\n",
    "lon_Lis = -9.1667\n",
    "lat_Rio = -22.908333\n",
    "lon_Rio = -43.196389"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_lon = get_closest(wave_height.longitude.data, lon_Lis)\n",
    "start_lat = get_closest(wave_height.latitude.data, lat_Lis)\n",
    "end_lon = get_closest(wave_height.longitude.data,lon_Rio)\n",
    "end_lat = get_closest(wave_height.latitude.data,lat_Rio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = (start_lat, start_lon)\n",
    "end = (end_lat, end_lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "land_mask = wave_height.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# mask for land areas\n",
    "land_mask[np.isnan(land_mask)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot optimal route\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "# Costs\n",
    "plt.imshow(land_mask, aspect='auto', cmap = \"RdBu\")\n",
    "\n",
    "plt.text(100, 600, \"Antlantic Ocean\", color = \"black\", fontfamily=\"serif\", fontsize = \"x-large\")\n",
    "plt.text(1, 200, \"South America\", color = \"black\", fontfamily=\"serif\", fontsize = \"x-large\")\n",
    "plt.text(380, 550, \"Africa\", color = \"black\", fontfamily=\"serif\", fontsize = \"x-large\")\n",
    "plt.title(\"Land mask\")\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wh_costs = wave_height.data\n",
    "wd_costs = wave_dir.data\n",
    "temp_costs = temp.data\n",
    "sal_costs = sal.data\n",
    "\n",
    "\n",
    "# Set NaN values to large wh_costs as the algorithm cannot handle NaNs\n",
    "wh_costs[np.isnan(wh_costs)] = np.nanmean(wh_costs) \n",
    "wd_costs[np.isnan(wd_costs)] = np.nanmean(wd_costs) \n",
    "temp_costs[np.isnan(temp_costs)] = np.nanmean(temp_costs) \n",
    "sal_costs[np.isnan(sal_costs)] = np.nanmean(sal_costs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization and normalization of weights\n",
    "def stand_and_norm (x):\n",
    "    # Standardization\n",
    "    x_stand = (x - np.mean(x)) / np.std(x)\n",
    "    # Normalization\n",
    "    x_norm = (x_stand - np.min(x_stand)) / (np.max(x_stand) - np.min(x_stand))\n",
    "    return x_norm\n",
    "\n",
    "wh_costs = stand_and_norm(wh_costs)\n",
    "wd_costs = stand_and_norm(wd_costs)\n",
    "temp_costs = stand_and_norm(temp_costs)\n",
    "sal_costs = stand_and_norm(sal_costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calulate costs based on linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Weight are taken from linear regression model\n",
    "speed = 0.14661334*wh_costs + 0.04047778*wd_costs + 0.13590311*temp_costs + 0.08404383*sal_costs\n",
    "\n",
    "# invert costs, because costs imitate speed \n",
    "inverted_speed = -1 * speed + np.abs(np.max(speed))\n",
    "\n",
    "# inverted_speed[inverted_speed == 0] = 2*np.max(inverted_speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# assign non-water areas high values\n",
    "inverted_speed = inverted_speed.compute()\n",
    "inverted_speed[land_mask ==1] = inverted_speed.max()\n",
    "\n",
    "wh_costs[land_mask ==1] = wh_costs.max()\n",
    "wd_costs[land_mask ==1] = wd_costs.max()\n",
    "temp_costs[land_mask ==1] = temp_costs.max()\n",
    "sal_costs[land_mask ==1] = sal_costs.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.graph import route_through_array\n",
    "\n",
    "# Calculate optimal route based on the minimum cost path\n",
    "\n",
    "# Optional parameters:\n",
    "# - fully_connected \n",
    "#     - False -> only axial moves are allowed\n",
    "#     - True  -> diagonal moves are allowed\n",
    "# - geometric \n",
    "#     - False -> minimum cost path\n",
    "#     - True  -> distance-weighted minimum cost path\n",
    "\n",
    "wh_indices, weight = route_through_array(wh_costs, start, end, fully_connected=True, geometric=True)\n",
    "wh_indices = np.stack(wh_indices, axis=-1)\n",
    "\n",
    "wd_indices, weight = route_through_array(wd_costs, start, end, fully_connected=True, geometric=True)\n",
    "wd_indices = np.stack(wd_indices, axis=-1)\n",
    "\n",
    "temp_indices, weight = route_through_array(temp_costs, start, end, fully_connected=True, geometric=True)\n",
    "temp_indices = np.stack(temp_indices, axis=-1)\n",
    "\n",
    "sal_indices, weight = route_through_array(sal_costs, start, end, fully_connected=True, geometric=True)\n",
    "sal_indices = np.stack(sal_indices, axis=-1)\n",
    "\n",
    "merged_indices, weight = route_through_array(inverted_speed, start, end, fully_connected=True, geometric=True)\n",
    "merged_indices = np.stack(merged_indices, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot optimal route\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "# Costs\n",
    "plt.imshow(inverted_speed, aspect='auto', vmin=np.min(inverted_speed), vmax=np.max(inverted_speed))\n",
    "\n",
    "# Routes\n",
    "plt.plot(wh_indices[1],wh_indices[0], 'red', label = \"wave height\")\n",
    "plt.plot(wd_indices[1],wd_indices[0], 'grey', label = \"wave direction\")\n",
    "plt.plot(temp_indices[1],temp_indices[0], 'orange', label = \"temperature\")\n",
    "plt.plot(sal_indices[1],sal_indices[0], 'cyan', label = \"salinity\")\n",
    "plt.plot(merged_indices[1],merged_indices[0], 'black', label = \"wave height + wave dir + temperature + salinity\")\n",
    "\n",
    "# Start/end points\n",
    "plt.plot(start_lon, start_lat, 'k^', markersize = 15)\n",
    "plt.text(start_lon - 50, start_lat - 10, \"Lisbon\", color = \"white\")\n",
    "plt.plot(end_lon, end_lat, 'k*', markersize=15)\n",
    "plt.text(end_lon + 20, end_lat, \"Rio de Janeiro\", color = \"white\")\n",
    "plt.title(\"Linear regression: Optimal routes (minimum cost path) from Lisbon to Rio de Janeiro\")\n",
    "plt.colorbar(label='Inverted speed over ground')\n",
    "plt.legend(loc = \"lower right\")\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calulate costs based on random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the numpy arrays\n",
    "wave_height_np = wh_costs.compute()\n",
    "wave_dir_np = wd_costs.compute()\n",
    "wave_height_np = temp_costs.compute()\n",
    "wave_height_np = sal_costs.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_height_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape 2d array to dataframes to apply the random forest\n",
    "wave_height_1d = pd.DataFrame(data = wh_costs.compute().ravel())\n",
    "wave_dir_1d = pd.DataFrame(data = wd_costs.compute().ravel())\n",
    "temp_1d = pd.DataFrame(data = temp_costs.compute().ravel())\n",
    "sal_1d = pd.DataFrame(data = sal_costs.compute().ravel())\n",
    "\n",
    "concat_costs = pd.concat([wave_height_1d,wave_dir_1d,temp_1d,sal_1d], axis = 1)\n",
    "concat_costs.columns = [\"Wave height\",\"Wave direction\",\"temperature\",\"salinity\"]\n",
    "\n",
    "for_pred = forest_model.predict(concat_costs)\n",
    "for_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert costs, because costs imitate speed \n",
    "inverted_speed_forest = -1 * for_pred + np.abs(np.max(for_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape speed costs to get back the map\n",
    "rf_speed = np.reshape(inverted_speed_forest,wave_height_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign non-water areas high values\n",
    "rf_speed[land_mask ==1] = np.max(rf_speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_speed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "land_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute route\n",
    "rf_indices, weight = route_through_array(rf_speed, start, end, fully_connected=True, geometric=True)\n",
    "rf_indices = np.stack(rf_indices, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot optimal route\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "# Costs\n",
    "plt.imshow(rf_speed, aspect='auto')\n",
    "\n",
    "plt.plot(rf_indices[1],rf_indices[0], 'red', label = \"fastest route from Lisbon to Rio\", lw = 2)\n",
    "\n",
    "# Start/end points\n",
    "plt.plot(start_lon, start_lat, 'k^', markersize = 15, color = \"white\")\n",
    "plt.text(start_lon - 50, start_lat - 10, \"Lisbon\", color = \"white\")\n",
    "plt.plot(end_lon, end_lat, 'k*', markersize=15)\n",
    "plt.text(end_lon + 20, end_lat, \"Rio de Janeiro\")\n",
    "plt.title(\"Random forest\")\n",
    "plt.legend(loc = \"lower right\")\n",
    "plt.colorbar(label='Inverted speed over ground')\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced solution: Calculate optimal route (minimum cost path) based on *multiple* variables and *multiple* days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show times for WAV\n",
    "time = phy_all.sel(time=~phy_all.get_index(\"time\").duplicated()).time\n",
    "\n",
    "rows = []\n",
    "\n",
    "for step in range(time.size):\n",
    "    \n",
    "    time_df_phy = str(time.values[step])\n",
    "    rows.append([step, time_df_phy])\n",
    "\n",
    "time_df_phy = pd.DataFrame(rows, columns = ['Step', 'Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show times for PHY\n",
    "time = wav_all.sel(time=~wav_all.get_index(\"time\").duplicated()).time\n",
    "\n",
    "rows = []\n",
    "\n",
    "for step in range(time.size):\n",
    "    \n",
    "    time_df_wav = str(time.values[step])\n",
    "    rows.append([step, time_df_wav])\n",
    "\n",
    "time_df_wav = pd.DataFrame(rows, columns = ['Step', 'Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the times that exist in both datasets (12:00:00 for the three days)\n",
    "common_time = np.intersect1d(time_df_wav['Time'], time_df_phy['Time'])\n",
    "\n",
    "# Create arrays with the corresponding steps\n",
    "# For waves forecast, this is: [3, 11, 19, 27]\n",
    "# For physics forecast, this is: [0, 1, 2, 3]\n",
    "time_wav = []\n",
    "time_phy = []\n",
    "\n",
    "for t in range(common_time.size):\n",
    "    time_wav.append(int(time_df_wav['Step'].loc[time_df_wav['Time'] == common_time[t]]))\n",
    "    time_phy.append(int(time_df_phy['Step'].loc[time_df_phy['Time'] == common_time[t]]))\n",
    "\n",
    "# Create time slices (start, end, step)\n",
    "step_wav = time_wav[1] - time_wav[0]\n",
    "time_slice_wav = slice(min(time_wav), max(time_wav) + 1, step_wav)\n",
    "step_phy = time_phy[1] - time_phy[0]\n",
    "time_slice_phy = slice(min(time_phy), max(time_phy) + 1, step_phy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract array from dataset to define the cost in the routing algorithm with a new time_slice\n",
    "# Wave height\n",
    "wave_height = wav_all.VHM0.isel(time=time_slice_wav, longitude=slice(lon_min, lon_max), latitude=slice(lat_min, lat_max))\n",
    "\n",
    "# Wave direction\n",
    "wave_dir = wav_all.VMDR.isel(time=time_slice_wav, longitude=slice(lon_min, lon_max), latitude=slice(lat_min, lat_max))\n",
    "\n",
    "# Temperature\n",
    "temp = phy_all.thetao.isel(time=time_slice_phy, longitude=slice(lon_min, lon_max), latitude=slice(lat_min, lat_max), depth = 0)\n",
    "\n",
    "# Salinity\n",
    "sal = phy_all.so.isel(time=time_slice_phy, longitude=slice(lon_min, lon_max), latitude=slice(lat_min, lat_max), depth = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate multidimensional costs\n",
    "wh_costs = wave_height\n",
    "wh_costs = wh_costs.fillna(np.nanmean(wh_costs))\n",
    "wh_costs = stand_and_norm(wh_costs)\n",
    "\n",
    "wd_costs = wave_dir\n",
    "wd_costs = wd_costs.fillna(np.nanmean(wd_costs))\n",
    "wd_costs = stand_and_norm(wd_costs)\n",
    "\n",
    "temp_costs = temp\n",
    "temp_costs = temp_costs.fillna(np.nanmean(temp_costs))\n",
    "temp_costs = temp_costs.reindex(latitude = wh_costs.latitude, method='nearest', tolerance=123).reindex(longitude = wh_costs.longitude, method='nearest', tolerance=123)\n",
    "temp_costs = stand_and_norm(temp_costs)\n",
    "\n",
    "sal_costs = sal\n",
    "sal_costs = sal_costs.fillna(np.nanmean(sal_costs))\n",
    "sal_costs = sal_costs.reindex(latitude = wh_costs.latitude, method='nearest', tolerance=123).reindex(longitude = wh_costs.longitude, method='nearest', tolerance=123)\n",
    "sal_costs = stand_and_norm(sal_costs)\n",
    "# Cost calulation using linear regression\n",
    "# costs = 0.14661334 * wh_costs + 0.04047778 * wd_costs + 0.13590311 * temp_costs + 0.08404383 * sal_costs\n",
    "# costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape 2d array to dataframes to apply the random forest\n",
    "wave_height_1d = pd.DataFrame(data = wh_costs.values.flatten())\n",
    "wave_dir_1d = pd.DataFrame(data = wd_costs.values.flatten())\n",
    "temp_1d = pd.DataFrame(data = temp_costs.values.flatten())\n",
    "sal_1d = pd.DataFrame(data = sal_costs.values.flatten())\n",
    "\n",
    "concat_costs = pd.concat([wave_height_1d,wave_dir_1d,temp_1d,sal_1d], axis = 1)\n",
    "concat_costs.columns = [\"Wave height\",\"Wave direction\",\"temperature\",\"salinity\"]\n",
    "\n",
    "# predict speed using random forest model\n",
    "costs = forest_model.predict(concat_costs)\n",
    "costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape speed costs to get back the map\n",
    "costs = np.reshape(costs,wh_costs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum costs for all timesteps\n",
    "costs_all_times = (costs[0]+costs[1]+costs[2])/3\n",
    "#costs_all_times = costs.sum(dim = 'time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data for specific timestep\n",
    "costs_day_1 = costs[0]\n",
    "costs_day_2 = costs[1]\n",
    "costs_day_3 = costs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign max values for land areas\n",
    "costs_day_1[land_mask == 1] = costs_day_1.max()\n",
    "costs_day_2[land_mask == 1] = costs_day_2.max()\n",
    "costs_day_3[land_mask == 1] = costs_day_3.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign non-water areas high values\n",
    "costs_all_times[land_mask ==1] = costs_all_times.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate optimal route for all days\n",
    "merged_indices_all_times, weight = route_through_array(costs_all_times, start, end, fully_connected=True, geometric=False)\n",
    "merged_indices_all_times = np.stack(merged_indices_all_times, axis=-1)\n",
    "\n",
    "# Calculate optimal route for day one\n",
    "merged_indices_day_one, weight = route_through_array(costs_day_1, start, end, fully_connected=True, geometric=False)\n",
    "merged_indices_day_one = np.stack(merged_indices_day_one, axis=-1)\n",
    "\n",
    "# Calculate optimal route for day two\n",
    "merged_indices_day_two, weight = route_through_array(costs_day_2, start, end, fully_connected=True, geometric=False)\n",
    "merged_indices_day_two = np.stack(merged_indices_day_two, axis=-1)\n",
    "\n",
    "# Calculate optimal route for day three\n",
    "merged_indices_day_three, weight = route_through_array(costs_day_3, start, end, fully_connected=True, geometric=False)\n",
    "merged_indices_day_three = np.stack(merged_indices_day_three, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot optimal route\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "# Costs\n",
    "plt.imshow(costs_all_times, aspect='auto')\n",
    "\n",
    "# Routes\n",
    "plt.plot(merged_indices_day_one[1], merged_indices_day_one[0], 'yellow', label = \"Day 1\")\n",
    "plt.plot(rf_indices[1],rf_indices[0], 'black', label = \"fastest route from Lisbon to Rio\", lw = 2)\n",
    "plt.plot(merged_indices_day_three[1], merged_indices_day_three[0], 'magenta', label = \"Day 3\")\n",
    "plt.plot(merged_indices_all_times[1], merged_indices_all_times[0], 'red', label = \"All days\")\n",
    "\n",
    "# Start/end points\n",
    "plt.plot(start_lon, start_lat, 'k^', markersize = 15)\n",
    "plt.text(start_lon - 50, start_lat - 10, \"Lisbon\", color = \"white\")\n",
    "plt.plot(end_lon, end_lat, 'k*', markersize = 15)\n",
    "plt.text(end_lon + 20, end_lat, \"Rio de Janeiro\")\n",
    "plt.title(\"Optimal routes (minimum cost path) from Lisbon to Rio de Janeiro based on RF\")\n",
    "plt.colorbar(label='Inverted speed over ground all days')\n",
    "plt.legend(loc = \"lower right\")\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "* https://levelup.gitconnected.com/dijkstras-shortest-path-algorithm-in-a-grid-eb505eb3a290\n",
    "* https://gist.github.com/mdsrosa/c71339cb23b <-- (Page not found..)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b9f0c1e033f7f4447e906abad9033ba03d46370fe52f38df008a2d6ba759c442"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
